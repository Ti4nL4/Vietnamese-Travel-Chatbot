#!/usr/bin/env python
# coding: utf-8

# In[1]:


from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader,PyPDFLoader, DirectoryLoader
from sentence_transformers import SentenceTransformer
from langchain.embeddings.base import Embeddings
import os

import re
def remove_metadata(text):
    # X√≥a ƒëo·∫°n c√≥ d·∫°ng metadata={...}
    cleaned_text = re.sub(r"metadata=\{.*?\}", "", text, flags=re.DOTALL)
    return cleaned_text.strip()

# Get the absolute path to the rag_data directory
current_dir = os.path.dirname(os.path.abspath(__file__))
rag_data_path = os.path.join(current_dir, "rag_data", "text")

loader = DirectoryLoader(
    rag_data_path,
    loader_cls=TextLoader,
    loader_kwargs={'autodetect_encoding': True},
)
documents = loader.load()

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=600,       # Gi·∫£m t·ª´ 1000 -> 600
    chunk_overlap=150,    # Gi·∫£m t·ª´ 200 -> 150 (25% chunk size)
    length_function=len,
    separators=[
        "\n\n",        # ∆Øu ti√™n 1: ƒêo·∫°n vƒÉn
        "\n",          # ∆Øu ti√™n 2: Xu·ªëng d√≤ng
        ".", "!", "?", # ∆Øu ti√™n 3: K·∫øt th√∫c c√¢u
        " ",           # ∆Øu ti√™n 4: T·ª´
        ""             # Fallback
    ]
)
# T√°ch vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n nh·ªè
texts = text_splitter.split_documents(documents)

model = SentenceTransformer('dangvantuan/vietnamese-document-embedding', trust_remote_code=True)

# Create a custom embeddings class
class VietnameseEmbeddings(Embeddings):
    def __init__(self, model):
        self.model = model

    def embed_documents(self, texts):
        """Embed a list of documents."""
        embeddings = self.model.encode(texts, normalize_embeddings=True)
        return embeddings.tolist()

    def embed_query(self, text):
        """Embed a query."""
        embedding = self.model.encode(text, normalize_embeddings=True)
        return embedding.tolist()

# Create embeddings instance
embeddings = VietnameseEmbeddings(model)

# T·∫°o c∆° s·ªü d·ªØ li·ªáu FAISS
db = FAISS.from_documents(
    texts,
    embeddings,
    distance_strategy="COSINE"  # Use cosine similarity for better performance
)
db.save_local("faiss_index")


# In[2]:


import ollama

def generate_response(prompt):
    messages = [
        {"role": "system", "content": "D·ª±a tr√™n th√¥ng tin sau ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c v√† ng·∫Øn g·ªçn. N·∫øu th√¥ng tin kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥."},
        {"role": "user", "content": prompt}
    ]
    print(len(prompt))
    response = ollama.chat(
        model='gemma:7b-instruct-v1.1-q4_K_M',
        messages=messages,
    )
    return response["message"]["content"]



# In[5]:


import time
import sys
from pg_query import process_query

def rag_pipeline(query):
    start_time = time.time()
    
    # Get documents with scores
    docs_and_scores = db.similarity_search_with_score(query, k=3)
    
    # Define similarity threshold
    SIMILARITY_THRESHOLD = 0.3  # Adjust this threshold based on your needs
    
    # Filter documents by relevance
    filtered_docs = []
    max_similarity = 0
    
    for doc, score in docs_and_scores:
        # Convert score to similarity (FAISS uses distance, so lower is better)
        similarity = 1 - score
        print(similarity)   
        max_similarity = max(max_similarity, similarity)
        
        if similarity > SIMILARITY_THRESHOLD:
            doc.page_content = remove_metadata(doc.page_content)
            filtered_docs.append(doc)
    
    print(f"üîç FAISS search m·∫•t: {time.time() - start_time:.2f} gi√¢y")
    print(f"Found {len(filtered_docs)} relevant documents")
    print(f"Max similarity score: {max_similarity:.4f}")
        
    # If no documents meet the threshold, try PostgreSQL
    if not filtered_docs:
        print("No documents meet similarity threshold, querying PostgreSQL...")
        try:
            postgres_result = process_query(query)
            if postgres_result and "Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£" not in postgres_result:
                print("Found results in PostgreSQL")
                return postgres_result, "sql"  # Return the result and the path used
            else:
                return "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p ƒë·ªÉ tr·∫£ l·ªùi!", "sql"  # Return the result and the path used
        except Exception as e:
            print(f"Error querying PostgreSQL: {str(e)}")
            return "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p ƒë·ªÉ tr·∫£ l·ªùi!", "sql"  # Return the result and the path used
        
    
    # Create context from filtered documents
    context = "\n\n".join([doc.page_content for doc in filtered_docs])
    
    # Improved prompt template
    prompt_template = """D·ª±a tr√™n th√¥ng tin sau ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c v√† ng·∫Øn g·ªçn.
    N·∫øu th√¥ng tin kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥.

    Th√¥ng tin tham kh·∫£o:
    {context}

    C√¢u h·ªèi: {query}

    Tr·∫£ l·ªùi:"""
    
    prompt = prompt_template.format(context=context, query=query)
    
    print(f"Context length: {len(context)} characters")
    sys.stdout.write(prompt)
    
    start_time = time.time()
    response = generate_response(prompt)
    print(f"ü§ñ AI generate m·∫•t: {time.time() - start_time:.2f} gi√¢y")
    return {
        "answer": response,
        "sources": [doc.metadata.get('source', 'unknown') for doc in filtered_docs],
        "path": "rag"
    }, "rag"
    # return response, "rag"  # Return the result and the path used

# V√≠ d·ª• s·ª≠ d·ª•ng
# query = "Nh·ªØng ƒëi·ªÉm du l·ªãch n·ªïi b·∫≠t ·ªü ƒê√† N·∫µng?"
# query = "Festval ph·ªü 2025 di·ªÖn ra khi n√†o?"
# query = "Doanh thu c·ªßa Du l·ªãch Hu·∫ø trong qu√Ω I nƒÉm 2025?"
# query = "Ph√π ƒëi√™u Kala N√∫i B√† l√† g√¨?"

# query = "Cho t√¥i bi·∫øt th√¥ng tin c·ªßa m·ªôt s·ªë h∆∞·ªõng d·∫´n vi√™n t·∫°i ƒëi·ªÉm du l·ªãch H·ªôi An"
# query = "Cho t√¥i bi·∫øt th√¥ng tin c·ªßa m·ªôt s·ªë h∆∞·ªõng d·∫´n vi√™n ·ªü c√≥ n∆°i c·∫•p th·∫ª ·ªü H·ªì Ch√≠ Minh"
# query = "Cho t√¥i bi·∫øt th√¥ng tin c·ªßa m·ªôt s·ªë n∆°i l∆∞u tr√∫ ·ªü H√† N·ªôi"


# print(rag_pipeline(query))


