{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/letien/DoAn/myenv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/letien/DoAn/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader,PyPDFLoader, DirectoryLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "import re\n",
    "def remove_metadata(text):\n",
    "    # X√≥a ƒëo·∫°n c√≥ d·∫°ng metadata={...}\n",
    "    cleaned_text = re.sub(r\"metadata=\\{.*?\\}\", \"\", text, flags=re.DOTALL)\n",
    "    return cleaned_text.strip()\n",
    "loader = DirectoryLoader(\n",
    "    \"rag_data/text\",\n",
    "    # glob=\"**/*.pdf\",  # This will load all PDF files in the directory and subdirectories\n",
    "    # loader_cls=PyPDFLoader,\n",
    "    loader_cls=TextLoader,\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,       # Gi·∫£m t·ª´ 1000 -> 600\n",
    "    chunk_overlap=150,    # Gi·∫£m t·ª´ 200 -> 150 (25% chunk size)\n",
    "    length_function=len,\n",
    "    separators=[\n",
    "        \"\\n\\n\",        # ∆Øu ti√™n 1: ƒêo·∫°n vƒÉn\n",
    "        \"\\n\",          # ∆Øu ti√™n 2: Xu·ªëng d√≤ng\n",
    "        \".\", \"!\", \"?\", # ∆Øu ti√™n 3: K·∫øt th√∫c c√¢u\n",
    "        \" \",           # ∆Øu ti√™n 4: T·ª´\n",
    "        \"\"             # Fallback\n",
    "    ]\n",
    ")\n",
    "# T√°ch vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n nh·ªè\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "model = SentenceTransformer('dangvantuan/vietnamese-document-embedding', trust_remote_code=True)\n",
    "\n",
    "# Create a custom embeddings class\n",
    "class VietnameseEmbeddings(Embeddings):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"Embed a list of documents.\"\"\"\n",
    "        embeddings = self.model.encode(texts, normalize_embeddings=True)\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"Embed a query.\"\"\"\n",
    "        embedding = self.model.encode(text, normalize_embeddings=True)\n",
    "        return embedding.tolist()\n",
    "\n",
    "# Create embeddings instance\n",
    "embeddings = VietnameseEmbeddings(model)\n",
    "\n",
    "# T·∫°o c∆° s·ªü d·ªØ li·ªáu FAISS\n",
    "db = FAISS.from_documents(\n",
    "    texts,\n",
    "    embeddings,\n",
    "    distance_strategy=\"COSINE\"  # Use cosine similarity for better performance\n",
    ")\n",
    "db.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def generate_response(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"D·ª±a tr√™n th√¥ng tin sau ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c v√† ng·∫Øn g·ªçn. N·∫øu th√¥ng tin kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    print(len(prompt))\n",
    "    response = ollama.chat(\n",
    "        model='llama3.1:8b-instruct-q4_K_M',\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6390332579612732\n",
      "0.5552563369274139\n",
      "0.32902228832244873\n",
      "üîç FAISS search m·∫•t: 0.22 gi√¢y\n",
      "Found 2 relevant documents\n",
      "Max similarity score: 0.6390\n",
      "Context length: 829 characters\n",
      "D·ª±a tr√™n th√¥ng tin sau ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c v√† ng·∫Øn g·ªçn.\n",
      "    N·∫øu th√¥ng tin kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥.\n",
      "\n",
      "    Th√¥ng tin tham kh·∫£o:\n",
      "    Ph√π ƒëi√™u Kala N√∫i B√† l√† t√°c ph·∫©m ƒëi√™u kh·∫Øc ƒë√° Champa v·ªõi ch·∫•t li·ªáu ƒë√° t√∫p Riolit ƒêaxit, cao 60cm, ƒë·∫ø r·ªông 44cm, d√†y 17cm, tr·ªçng l∆∞·ª£ng 105,5kg.\n",
      "\n",
      "Ph√π ƒëi√™u c√πng v·ªõi c√°c hi·ªán v·∫≠t kh√°c g·∫Øn li·ªÅn v·ªõi ki·∫øn tr√∫c ƒë·ªÅn th√°p n√†y gi√∫p cho c√°c nh√† khoa h·ªçc x√°c ƒë·ªãnh ƒë∆∞·ª£c c√°c gi√° tr·ªã cƒÉn b·∫£n c·ªßa di v·∫≠t n√†y nh∆∞ t√≠nh nguy√™n g·ªëc, ni√™n ƒë·∫°i c·ªßa di t√≠ch v√† di v·∫≠t c≈©ng nh∆∞ x√°c ƒë·ªãnh ƒë∆∞·ª£c quy m√¥, t·∫ßm v√≥c c·ªßa di t√≠ch N√∫i B√†, trong ph·ª©c h·ª£p c√°c di t√≠ch ChƒÉm ·ªü Ph√∫ Y√™n n√≥i ri√™ng v√† di t√≠ch Champa tr√™n d·ªçc duy√™n h·∫£i mi·ªÅn Trung Vi·ªát Nam.\n",
      "\n",
      "Theo B·∫£o t√†ng Ph√∫ Y√™n, ph√π ƒëi√™u Kala N√∫i B√† c√≥ ni√™n ƒë·∫°i th·∫ø k·ª∑ XIV, ƒë∆∞·ª£c ph√°t hi·ªán v√†o nƒÉm 1993 trong cu·ªôc khai qu·∫≠t di t√≠ch N√∫i B√† ·ªü x√£ H√≤a Phong, huy·ªán T√¢y H√≤a (Ph√∫ Y√™n).\n",
      "\n",
      "Ph√π ƒëi√™u Kala N√∫i B√† l√† t√°c ph·∫©m ƒëi√™u kh·∫Øc ƒë√° Champa v·ªõi ch·∫•t li·ªáu ƒë√° t√∫p Riolit ƒêaxit, cao 60cm, ƒë·∫ø r·ªông 44cm, d√†y 17cm, tr·ªçng l∆∞·ª£ng 105,5kg.\n",
      "\n",
      "    C√¢u h·ªèi: Ph√π ƒëi√™u Kala N√∫i B√† l√† g√¨?\n",
      "\n",
      "    Tr·∫£ l·ªùi:1054\n",
      "ü§ñ AI generate m·∫•t: 19.66 gi√¢y\n",
      "Ph√π ƒëi√™u Kala N√∫i B√† l√† t√°c ph·∫©m ƒëi√™u kh·∫Øc ƒë√° Champa v·ªõi ch·∫•t li·ªáu ƒë√° t√∫p Riolit ƒêaxit.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "from pg_query import process_query\n",
    "\n",
    "def rag_pipeline(query):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get documents with scores\n",
    "    docs_and_scores = db.similarity_search_with_score(query, k=3)\n",
    "    \n",
    "    # Define similarity threshold\n",
    "    SIMILARITY_THRESHOLD = 0.5  # Adjust this threshold based on your needs\n",
    "    \n",
    "    # Filter documents by relevance\n",
    "    filtered_docs = []\n",
    "    max_similarity = 0\n",
    "    \n",
    "    for doc, score in docs_and_scores:\n",
    "        # Convert score to similarity (FAISS uses distance, so lower is better)\n",
    "        similarity = 1 - score\n",
    "        print(similarity)   \n",
    "        max_similarity = max(max_similarity, similarity)\n",
    "        \n",
    "        if similarity > SIMILARITY_THRESHOLD:\n",
    "            doc.page_content = remove_metadata(doc.page_content)\n",
    "            filtered_docs.append(doc)\n",
    "    \n",
    "    print(f\"üîç FAISS search m·∫•t: {time.time() - start_time:.2f} gi√¢y\")\n",
    "    print(f\"Found {len(filtered_docs)} relevant documents\")\n",
    "    print(f\"Max similarity score: {max_similarity:.4f}\")\n",
    "        \n",
    "    # If no documents meet the threshold, try PostgreSQL\n",
    "    if not filtered_docs:\n",
    "        print(\"No documents meet similarity threshold, querying PostgreSQL...\")\n",
    "        try:\n",
    "            postgres_result = process_query(query)\n",
    "            if postgres_result and \"Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£\" not in postgres_result:\n",
    "                print(\"Found results in PostgreSQL\")\n",
    "                return postgres_result  # Return PostgreSQL results directly\n",
    "            else:\n",
    "                return \"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p ƒë·ªÉ tr·∫£ l·ªùi!\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying PostgreSQL: {str(e)}\")\n",
    "            return \"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p ƒë·ªÉ tr·∫£ l·ªùi!\"\n",
    "        \n",
    "    \n",
    "    # Create context from filtered documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in filtered_docs])\n",
    "    \n",
    "    # Improved prompt template\n",
    "    prompt_template = \"\"\"D·ª±a tr√™n th√¥ng tin sau ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c v√† ng·∫Øn g·ªçn.\n",
    "    N·∫øu th√¥ng tin kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥.\n",
    "\n",
    "    Th√¥ng tin tham kh·∫£o:\n",
    "    {context}\n",
    "\n",
    "    C√¢u h·ªèi: {query}\n",
    "\n",
    "    Tr·∫£ l·ªùi:\"\"\"\n",
    "    \n",
    "    prompt = prompt_template.format(context=context, query=query)\n",
    "    \n",
    "    print(f\"Context length: {len(context)} characters\")\n",
    "    sys.stdout.write(prompt)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = generate_response(prompt)\n",
    "    print(f\"ü§ñ AI generate m·∫•t: {time.time() - start_time:.2f} gi√¢y\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng\n",
    "# query = \"Nh·ªØng ƒëi·ªÉm du l·ªãch n·ªïi b·∫≠t ·ªü ƒê√† N·∫µng?\"\n",
    "# query = \"Festval ph·ªü 2025 di·ªÖn ra khi n√†o?\"\n",
    "# query = \"Doanh thu c·ªßa Du l·ªãch Hu·∫ø trong qu√Ω I nƒÉm 2025?\"\n",
    "query = \"Ph√π ƒëi√™u Kala N√∫i B√† l√† g√¨?\"\n",
    "\n",
    "# query = \"Cho t√¥i bi·∫øt th√¥ng tin c·ªßa m·ªôt s·ªë h∆∞·ªõng d·∫´n vi√™n t·∫°i ƒëi·ªÉm du l·ªãch H·ªôi An\"\n",
    "# query = \"Cho t√¥i bi·∫øt th√¥ng tin c·ªßa m·ªôt s·ªë h∆∞·ªõng d·∫´n vi√™n ·ªü c√≥ n∆°i c·∫•p th·∫ª ·ªü H·ªì Ch√≠ Minh\"\n",
    "# query = \"Cho t√¥i bi·∫øt th√¥ng tin c·ªßa m·ªôt s·ªë n∆°i l∆∞u tr√∫ ·ªü H√† N·ªôi\"\n",
    "\n",
    "\n",
    "print(rag_pipeline(query))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
